
quarkus.langchain4j.mcp.memory.transport-type=stdio
quarkus.langchain4j.mcp.memory.command=npm,exec,@modelcontextprotocol/server-memory@0.6.2

quarkus.langchain4j.mcp.googlemaps.transport-type=stdio
quarkus.langchain4j.mcp.googlemaps.command=npm,exec,@modelcontextprotocol/server-google-maps@0.6.2

#
# If you want to use this extension you'll need to install it locally
# by following instructions here: https://github.com/nspady/google-calendar-mcp
# and don't forget to create gcloud app, put the keys and IDs in your
# .env file (or in this file), and generate refresh tokens as specified in
# the docs.
#
# quarkus.langchain4j.mcp.googlecal.transport-type=stdio
# quarkus.langchain4j.mcp.googlecal.command=node,google-calendar-mcp/build/index.js
#
# These should go in your .env file (and not checked into SCM):
# quarkus.langchain4j.mcp.googlecal.environment.GOOGLE_CLIENT_ID=xxxxx
# quarkus.langchain4j.mcp.googlecal.environment.GOOGLE_CLIENT_SECRET=xxxxxx
# quarkus.langchain4j.mcp.googlecal.environment.GOOGLE_REDIRECT_URI=http://localhost
# quarkus.langchain4j.mcp.googlecal.environment.GOOGLE_REFRESH_TOKEN=xxxxx
#

quarkus.langchain4j.mcp.bravesearch.transport-type=stdio
quarkus.langchain4j.mcp.bravesearch.command=npm,exec,@modelcontextprotocol/server-brave-search@0.6.2

quarkus.langchain4j.mcp.slack.transport-type=stdio
quarkus.langchain4j.mcp.slack.command=npm,exec,@modelcontextprotocol/server-slack@2025.1.17

quarkus.langchain4j.mcp.time.transport-type=stdio
quarkus.langchain4j.mcp.time.command=uvx,mcp-server-time,--local-timezone,America/New_York

quarkus.langchain4j.chat-model.provider=ollama

quarkus.langchain4j.ollama.base-url=http://localhost:11434
quarkus.langchain4j.ollama.chat-model.model-id=qwen2.5-coder
quarkus.langchain4j.ollama.log-requests=true
quarkus.langchain4j.ollama.log-responses=true
quarkus.langchain4j.ollama.timeout=5m

quarkus.langchain4j.openai.chat-model.temperature=0.8
quarkus.langchain4j.openai.chat-model.model-name=gpt-4o-mini
quarkus.langchain4j.openai.log-requests=true
quarkus.langchain4j.openai.log-responses=true
quarkus.langchain4j.openai.timeout=3m

quarkus.log.category.\"dev.langchain4j\".level=DEBUG
quarkus.log.category.\"io.quarkiverse.langchain4j\".level=DEBUG

quarkus.langchain4j.anthropic.chat-model.model-name=claude-3-5-sonnet-20241022
quarkus.langchain4j.anthropic.log-requests=true
quarkus.langchain4j.anthropic.log-responses=true
quarkus.langchain4j.anthropic.timeout=3m

# Comment this out to enable the LGTM observability stack
%dev.quarkus.observability.enabled=false
%dev.quarkus.datasource.jdbc.telemetry=false
%dev.quarkus.otel.logs.enabled=false
%dev.quarkus.otel.traces.enabled=false
%dev.quarkus.otel.enabled=false
%dev.quarkus.otel.logs.handler.enabled=false

# Uncomment this out to enable the LGTM observability stack
# %dev.quarkus.otel.exporter.otlp.traces.headers=authorization=Bearer my_secret
# %dev.quarkus.log.console.format=%d{HH:mm:ss} %-5p traceId=%X{traceId}, parentId=%X{parentId}, spanId=%X{spanId}, sampled=%X{sampled} [%c{2.}] (%t) %s%e%n
# %dev.quarkus.datasource.jdbc.telemetry=true
# %dev.quarkus.otel.logs.enabled=true
# %dev.quarkus.otel.traces.enabled=true
# %dev.quarkus.observability.enabled=true